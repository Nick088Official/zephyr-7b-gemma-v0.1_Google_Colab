{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nick088Official/zephyr-7b-gemma-v0.1_Google_Colab/blob/main/zephyr-7b-gemma-v0.1_Manual.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run zephyr-7b-gemma-v0.1 AI Model (Finetuned Version of Google Gemma 7B) NO WEB UI\n",
        "\n",
        "Model Used: https://huggingface.co/HuggingFaceH4/zephyr-7b-gemma-v0.1\n",
        "\n",
        "Google Colab Made by [Nick088](https://linktr.ee/Nick088)"
      ],
      "metadata": {
        "id": "rhAucpX5Li7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install & Load Dependencies, Model\n",
        "\n",
        "#@markdown If you wanna use GPU (faster, max 12 free hours daily limit): Set the Video Card from Edit -> Notebook Settings -> T4 GPU OR Any other GPUs based on your Google Colab Subscription\n",
        "!pip install einops\n",
        "!pip install accelerate\n",
        "import torch\n",
        "from IPython.display import clear_output\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    print(\"Using GPU\")\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    print(\"Detected CPU, please change to GPU or it won't work.\")\n",
        "\n",
        "# Now you can use 'device' for your PyTorch operations\n",
        "\n",
        "torch.set_default_device(device)\n",
        "\n",
        "#@markdown The normal version is the official one (PAID GPU REQUIRED), the other ones are GGUF quantized (WORK FOR FREE COLAB TOO), compressed to consume less ram, see more info about them in the [Hugging Face repo of zephyr-7b-gemma-v0.1 GGUF](https://huggingface.co/LoneStriker/zephyr-7b-gemma-v0.1-GGUF)\n",
        "\n",
        "model_version = \"Q3_K_L\" #@param ['normal', 'Q3_K_L', 'Q5_K_M', 'Q6_K', 'Q8_0']\n",
        "\n",
        "if model_version == \"normal\":\n",
        "  from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "  if device == \"cuda\":\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceH4/zephyr-7b-gemma-v0.1\", torch_dtype=\"auto\", trust_remote_code=True)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-gemma-v0.1\", trust_remote_code=True)\n",
        "  else:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceH4/zephyr-7b-gemma-v0.1\", torch_dtype=torch.float32, device_map=\"cpu\", trust_remote_code=True)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-gemma-v0.1\", trust_remote_code=True)\n",
        "else:\n",
        "  !nvidia-smi\n",
        "  if device == \"cuda\":\n",
        "    !CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python\n",
        "  else:\n",
        "    !pip install llama-cpp-python\n",
        "  from huggingface_hub import hf_hub_download\n",
        "  from llama_cpp import Llama\n",
        "  model_name = \"LoneStriker/zephyr-7b-gemma-v0.1-GGUF\"\n",
        "  model_file = f\"zephyr-7b-gemma-v0.1-{model_version}.gguf\"\n",
        "  model_path = hf_hub_download(model_name,\n",
        "                             filename=model_file,\n",
        "                             local_dir='/content')\n",
        "  llm = Llama(model_path=model_path,\n",
        "            n_gpu_layers=-1)\n",
        "\n",
        "clear_output()\n",
        "print(f\"Downladed zephyr-7b-gemma-v0.1 {model_version}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "livUz3tn2uWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run Inference\n",
        "#@markdown Your Prompt\n",
        "user_prompt = \"What is finetuning?\" #@param {type:\"string\"}\n",
        "\n",
        "# Additional inputs\n",
        "#@markdown Add your system prompt (prompt to personalize the AI) here or leave it empty if you wanna use the AI normally\n",
        "system_prompt = \"You are ShortAI, write short but concise responses\"  #@param {type:\"string\"}\n",
        "#@markdown The maximum number of tokens that the model will generate in response to your input\n",
        "max_new_tokens = 3099 #@param {type:\"slider\", min:256, max:8192, step:1}\n",
        "#@markdown Penalize repeated tokens, so make the AI repeat less of itself\n",
        "repetition_penalty = 1.2 #@param {type:\"slider\", min:0.0, max:2, step:0.1}\n",
        "\n",
        "# Concatenate system and user prompts\n",
        "full_prompt = system_prompt + user_prompt\n",
        "\n",
        "if model_version == \"normal\":\n",
        "  # Tokenize the combined prompt\n",
        "  inputs = tokenizer(full_prompt, return_tensors=\"pt\", return_attention_mask=False)\n",
        "\n",
        "  # Generate text based on the combined prompt and additional inputs\n",
        "  outputs = model.generate(\n",
        "      **inputs,\n",
        "      max_new_tokens=max_new_tokens,\n",
        "      repetition_penalty=repetition_penalty\n",
        "      )\n",
        "\n",
        "  text = tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "  # Extract the generated text from the model output\n",
        "  text = text[len(system_prompt):].strip()  # Remove the system prompt from the generated text\n",
        "  clear_output()\n",
        "  print(text)\n",
        "else:\n",
        "  response = llm(full_prompt, max_tokens=max_new_tokens, repeat_penalty=repetition_penalty)\n",
        "  clear_output()\n",
        "  print(response['choices'][0]['text'])"
      ],
      "metadata": {
        "id": "4901EHafGkhK",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}