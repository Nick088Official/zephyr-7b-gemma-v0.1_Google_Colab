{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nick088Official/zephyr-7b-gemma-v0.1_Google_Colab/blob/main/zephyr-7b-gemma-v0.1_UI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run zephyr-7b-gemma-v0.1 AI Model (Finetuned Version of Google Gemma 7B) WEB UI\n",
        "\n",
        "Model Used: https://huggingface.co/HuggingFaceH4/zephyr-7b-gemma-v0.1\n",
        "\n",
        "Google Colab Made by [Nick088](https://linktr.ee/Nick088)"
      ],
      "metadata": {
        "id": "rhAucpX5Li7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install & Load Dependencies, Model\n",
        "\n",
        "#@markdown If you wanna use GPU (faster, max 12 free hours daily limit): Set the Video Card from Edit -> Notebook Settings -> T4 GPU OR Any other GPUs based on your Google Colab Subscription\n",
        "!pip install einops\n",
        "!pip install accelerate\n",
        "import torch\n",
        "from IPython.display import clear_output\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    print(\"Using GPU\")\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    print(\"Detected CPU, please change to GPU or it won't work.\")\n",
        "\n",
        "# Now you can use 'device' for your PyTorch operations\n",
        "\n",
        "torch.set_default_device(device)\n",
        "\n",
        "#@markdown The normal version is the official one (PAID GPU REQUIRED), the other ones are GGUF quantized (WORK FOR FREE COLAB TOO), compressed to consume less ram, see more info about them in the [Hugging Face repo of zephyr-7b-gemma-v0.1 GGUF](https://huggingface.co/LoneStriker/zephyr-7b-gemma-v0.1-GGUF)\n",
        "\n",
        "model_version = \"Q3_K_L\" #@param ['normal', 'Q3_K_L', 'Q5_K_M', 'Q6_K', 'Q8_0']\n",
        "\n",
        "if model_version == \"normal\":\n",
        "  from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "  if device == \"cuda\":\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceH4/zephyr-7b-gemma-v0.1\", torch_dtype=\"auto\", trust_remote_code=True)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-gemma-v0.1\", trust_remote_code=True)\n",
        "  else:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceH4/zephyr-7b-gemma-v0.1\", torch_dtype=torch.float32, device_map=\"cpu\", trust_remote_code=True)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-gemma-v0.1\", trust_remote_code=True)\n",
        "else:\n",
        "  !nvidia-smi\n",
        "  if device == \"cuda\":\n",
        "    !CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python\n",
        "  else:\n",
        "    !pip install llama-cpp-python\n",
        "  from huggingface_hub import hf_hub_download\n",
        "  from llama_cpp import Llama\n",
        "  model_name = \"LoneStriker/zephyr-7b-gemma-v0.1-GGUF\"\n",
        "  model_file = f\"zephyr-7b-gemma-v0.1-{model_version}.gguf\"\n",
        "  model_path = hf_hub_download(model_name,\n",
        "                             filename=model_file,\n",
        "                             local_dir='/content')\n",
        "  llm = Llama(model_path=model_path,\n",
        "            n_gpu_layers=-1)\n",
        "\n",
        "clear_output()\n",
        "print(f\"Downladed zephyr-7b-gemma-v0.1 {model_version}\")"
      ],
      "metadata": {
        "id": "GyK68jfLe5gy",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run zephyr-7b-gemma-v0.1 UI\n",
        "\n",
        "def answer(user_prompt, system_prompt, max_new_tokens, repetition_penalty):\n",
        "\n",
        "  if model_version == \"normal\":\n",
        "    # Tokenize the combined prompt\n",
        "    inputs = tokenizer(full_prompt, return_tensors=\"pt\", return_attention_mask=False)\n",
        "\n",
        "    # Generate text based on the combined prompt and additional inputs\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        repetition_penalty=repetition_penalty\n",
        "        )\n",
        "\n",
        "    text = tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "    # Extract the generated text from the model output\n",
        "    text = text[len(system_prompt):].strip()  # Remove the system prompt from the generated text\n",
        "    clear_output()\n",
        "    return text\n",
        "  else:\n",
        "    response = llm(full_prompt, max_tokens=max_new_tokens, repeat_penalty=repetition_penalty)\n",
        "    clear_output()\n",
        "    response = response['choices'][0]['text'])\n",
        "   return response\n",
        "\n",
        "# Define the gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=answer,\n",
        "    inputs=[\n",
        "        gr.Textbox(lines=2, label=\"Your Prompt\"),\n",
        "        gr.Textbox(lines=2, label=\"System Prompt\"),\n",
        "        gr.Slider(value=512, minimum=256, maximum=8000, step=1, interactive=True, label=\"Max Tokens\"),\n",
        "        gr.Slider(value=1.2, minimum=0.0, maximum=2.0, step=0.1, interactive=True, label=\"Repetition Penalty\"),\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=f\"zephyr-7b-gemma-v0.1 {model_version}\",\n",
        ")\n",
        "\n",
        "\n",
        "# Launch the gradio interface\n",
        "interface.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "4901EHafGkhK",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}